<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Text-to-Image Generation with Adapter Conditioning

## Overview

[T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.08453) by Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, Xiaohu Qie.

Using the pretrained models we can provide control images (for example, a depth map) to control Stable Diffusion text-to-image generation so that it follows the structure of the depth image and fills in the details.

The abstract of the paper is the following:

*The incredible generative ability of large-scale text-to-image (T2I) models has demonstrated strong power of learning complex structures and meaningful semantics. However, relying solely on text prompts cannot fully take advantage of the knowledge learned by the model, especially when flexible and accurate structure control is needed. In this paper, we aim to ``dig out" the capabilities that T2I models have implicitly learned, and then explicitly use them to control the generation more granularly. Specifically, we propose to learn simple and small T2I-Adapters to align internal knowledge in T2I models with external control signals, while freezing the original large T2I models. In this way, we can train various adapters according to different conditions, and achieve rich control and editing effects. Further, the proposed T2I-Adapters have attractive properties of practical value, such as composability and generalization ability. Extensive experiments demonstrate that our T2I-Adapter has promising generation quality and a wide range of applications.*

## Available Pipelines:

| Pipeline | Tasks | Demo
|---|---|:---:|
| [StableDiffusionAdapterPipeline](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_adapter.py) | *Text-to-Image Generation with T2I-Adapter Conditioning* | -

## Usage example

In the following we give a simple example of how to use a *Adapter* checkpoint with Diffusers for inference.
The inference pipeline is the same for all pipelines:

 1. Take an image and run it through a pre-conditioning processor to obtain *control image*.
 2. Run the pre-processed *control image* and *prompt* through the [`StableDiffusionAdapterPipeline`].

Let's have a look at a simple example using the [Color Adapter](https://huggingface.co/RzZ/sd-v1-4-adapter-color).

```python
from diffusers.utils import load_image

image = load_image("https://huggingface.co/RzZ/sd-v1-4-adapter-color/resolve/main/color_ref.png")
```

![img](https://huggingface.co/RzZ/sd-v1-4-adapter-color/resolve/main/color_ref.png)


Then we can create our color palette by simply resize it to 8 by 8 pixels then scale it back to original size.

```python
from PIL import Image

color_palette = image.resize((8, 8))
color_palette = color_palette.resize((512, 512), resample=Image.Resampling.NEAREST)
```

Let's take a look at the processed image.

![img](https://huggingface.co/RzZ/sd-v1-4-adapter-color/resolve/main/color_palette.png)


After we having `color_palette` in hand, we can create the [`StableDiffusionAdapterPipeline`] with pretrained checkpoint.

```py
import torch
from diffusers import StableDiffusionAdapterPipeline, Adapter

adapter = Adapter.from_pretrained("RzZ/sd-v1-4-adapter-color")
pipe = StableDiffusionAdapterPipeline.from_pretrained(
    "CompVis/stable-diffusion-v1-4",
    adapter=adapter,
    torch_dtype=torch.float16,
)
pipe.to("cuda")
```

And finally we feed the data to the pipelien and wait for the result!

```py
# fix the random seed, so you will get the same result as the example
generator = torch.manual_seed(7)

out_image = pipe(
    ["At night, glowing cubes in front of the beach"],
    image=[color_palette],
    generator=generator,
).images[0]
```

This should take only few seconds on GPU (depending on hardware). The output image then looks as follows:

![img](https://huggingface.co/RzZ/sd-v1-4-adapter-color/resolve/main/color_output.png)


**Note**: To see how to run all other Adapter checkpoints, please have a look at [T2I-Adapter with Stable Diffusion 1.4](#t2i-adapter-with-stable-diffusion-1.4)

<!-- TODO: add space -->

## Available checkpoints

Adapter requires a *control image* in addition to the text-to-image *prompt*. 
Each pretrained model is trained using a different conditioning method that requires different images for conditioning the generated outputs. For example, Canny edge conditioning requires the control image to be the output of a Canny filter, while depth conditioning requires the control image to be a depth map. See the overview and image examples below to know more.

All official checkpoints can be found under the authors' namespace [TencentARC/T2I-Adapter](https://huggingface.co/TencentARC/T2I-Adapter/tree/main/models).

### T2I-Adapter with Stable Diffusion 1.4

| Model Name | Control Image Overview| Control Image Example | Generated Image Example |
|---|---|---|---|
|[RzZ/sd-v1.4-adapter-color](https://huggingface.co/RzZ/sd-v1-4-adapter-color/)<br/> *Trained with spatial color palette* | A image with 8x8 color palette.|<a href="https://huggingface.co/RzZ/sd-v1-4-adapter-color/resolve/main/sample_input.png"><img width="64" style="margin:0;padding:0;" src="https://huggingface.co/RzZ/sd-v1-4-adapter-color/resolve/main/sample_input.png"/></a>|<a href="https://huggingface.co/RzZ/sd-v1-4-adapter-color/resolve/main/sample_output.png"><img width="64" src="https://huggingface.co/RzZ/sd-v1-4-adapter-color/resolve/main/sample_output.png"/></a>|
|[RzZ/sd-v1.4-adapter-canny](https://huggingface.co/RzZ/sd-v1-4-adapter-canny)<br/> *Trained with canny edge detection* | A monochrome image with white edges on a black background.|<a href="https://huggingface.co/RzZ/sd-v1-4-adapter-canny/resolve/main/sample_input.png"><img width="64" style="margin:0;padding:0;" src="https://huggingface.co/RzZ/sd-v1-4-adapter-canny/resolve/main/sample_input.png"/></a>|<a href="https://huggingface.co/RzZ/sd-v1-4-adapter-canny/resolve/main/sample_output.png"><img width="64" src="https://huggingface.co/RzZ/sd-v1-4-adapter-canny/resolve/main/sample_output.png"/></a>|
|[RzZ/sd-v1.4-adapter-sketch](https://huggingface.co/RzZ/sd-v1-4-adapter-sketch)<br/> *Trained with [PidiNet](https://github.com/zhuoinoulu/pidinet) edge detection* | A hand-drawn monochrome image with white outlines on a black background.|<a href="https://huggingface.co/RzZ/sd-v1-4-adapter-sketch/resolve/main/sample_input.png"><img width="64" style="margin:0;padding:0;" src="https://huggingface.co/RzZ/sd-v1-4-adapter-sketch/resolve/main/sample_input.png"/></a>|<a href="https://huggingface.co/RzZ/sd-v1-4-adapter-sketch/resolve/main/sample_output.png"><img width="64" src="https://huggingface.co/RzZ/sd-v1-4-adapter-sketch/resolve/main/sample_output.png"/></a>|
|[RzZ/sd-v1.4-adapter-depth](https://huggingface.co/RzZ/sd-v1-4-adapter-depth)<br/> *Trained with Midas depth estimation*  | A grayscale image with black representing deep areas and white representing shallow areas.|<a href="https://huggingface.co/RzZ/sd-v1-4-adapter-depth/resolve/main/sample_input.png"><img width="64" src="https://huggingface.co/RzZ/sd-v1-4-adapter-depth/resolve/main/sample_input.png"/></a>|<a href="https://huggingface.co/RzZ/sd-v1-4-adapter-depth/resolve/main/sample_output.png"><img width="64" src="https://huggingface.co/RzZ/sd-v1-4-adapter-depth/resolve/main/sample_output.png"/></a>|
|[RzZ/sd-v1.4-adapter-openpose](https://huggingface.co/RzZ/sd-v1-4-adapter-openpose)<br/> *Trained with OpenPose bone image*  | A [OpenPose bone](https://github.com/CMU-Perceptual-Computing-Lab/openpose) image.|<a href="https://huggingface.co/RzZ/sd-v1-4-adapter-openpose/resolve/main/sample_input.png"><img width="64" src="https://huggingface.co/RzZ/sd-v1-4-adapter-openpose/resolve/main/sample_input.png"/></a>|<a href="https://huggingface.co/RzZ/sd-v1-4-adapter-openpose/resolve/main/sample_output.png"><img width="64" src="https://huggingface.co/RzZ/sd-v1-4-adapter-openpose/resolve/main/sample_output.png"/></a>|
|[RzZ/sd-v1.4-adapter-keypose](https://huggingface.co/RzZ/sd-v1-4-adapter-keypose)<br/> *Trained with mmpose skeleton image*  | A [mmpose skeleton](https://github.com/open-mmlab/mmpose) image.|<a href="https://huggingface.co/RzZ/sd-v1-4-adapter-keypose/resolve/main/sample_input.png"><img width="64" src="https://huggingface.co/RzZ/sd-v1-4-adapter-keypose/resolve/main/sample_input.png"/></a>|<a href="https://huggingface.co/RzZ/sd-v1-4-adapter-keypose/resolve/main/sample_output.png"><img width="64" src="https://huggingface.co/RzZ/sd-v1-4-adapter-keypose/resolve/main/sample_output.png"/></a>|
|[RzZ/sd-v1.4-adapter-seg](https://huggingface.co/RzZ/sd-v1-4-adapter-seg)<br/>*Trained with semantic segmentation*  | An [custom](https://github.com/TencentARC/T2I-Adapter/discussions/25) segmentation protocol image.|<a href="https://huggingface.co/RzZ/sd-v1-4-adapter-seg/resolve/main/sample_input.png"><img width="64" src="https://huggingface.co/RzZ/sd-v1-4-adapter-seg/resolve/main/sample_input.png"/></a>|<a href="https://huggingface.co/RzZ/sd-v1-4-adapter-seg/resolve/main/sample_output.png"><img width="64" src="https://huggingface.co/RzZ/sd-v1-4-adapter-seg/resolve/main/sample_output.png"/></a> |

## Mix and match multiple adapters

[`StableDiffusionAdapterPipeline`] also support using multiple type of *control image* at once with combination with [`MultiAdapter`].
Here is a example of using keypose adapter for character posture control and depth adapter for outlining background.

Just like the previous example, we will first prepare the *control image* for inference. One big difference when using [`MultiAdapter`] is that the *control image* we will send to pipeline is 
combined from multiple images. In this example we stack two 3 channels RGB image(`cond_keypose`, `cond_depth`) together to create a 6 channels image tensor(`cond`).

```py
import torch
from PIL import Image
from diffusers.utils import load_image

cond_keypose = load_image(
    "https://huggingface.co/RzZ/sd-v1-4-adapter-keypose-depth/resolve/main/sample_input_keypose.png"
)
cond_depth = load_image("https://huggingface.co/RzZ/sd-v1-4-adapter-keypose-depth/resolve/main/sample_input_depth.png")
cond = [[cond_keypose, cond_depth]]

prompt = ["A man waling in an office room with nice view"]
```

Two *control image* should look like follows:

![img](https://huggingface.co/RzZ/sd-v1-4-adapter-keypose-depth/resolve/main/sample_input_keypose.png)
![img](https://huggingface.co/RzZ/sd-v1-4-adapter-keypose-depth/resolve/main/sample_input_depth.png)


Now we can using `from_adapters` method combine keypose and depth adapter into one, then pass our newly created [`MultiAdapter`] to 
[`StableDiffusionAdapterPipeline`]. You can also play around the value of `adapter_weights` to balance the control between adapters.

```py
from diffusers import StableDiffusionAdapterPipeline, MultiAdapter

adapters = [
    Adapter.from_pretrained("RzZ/sd-v1-4-adapter-keypose"),
    Adapter.from_pretrained("RzZ/sd-v1-4-adapter-depth"),
]
adapters = adapters.to(torch.float16)

pipe = StableDiffusionAdapterPipeline.from_pretrained(
    "CompVis/stable-diffusion-v1-4",
    torch_dtype=torch.float16,
    adapter=adapters,
)

images = pipe(prompt, cond)
```

After prompt and image is processed by pipeline we should get the result looks like:

![img](https://huggingface.co/RzZ/sd-v1-4-adapter-keypose-depth/resolve/main/sample_output.png)


## Difference with ControlNet

Foundatly T2I-Adapter is try to achieve the same goal as ControlNet with simliar approch, the main difference between the two is:
1. Unlike ControlNet which "fork" the encoder part of the UNet to process conditioning signal, T2I-Adapter use a light weight network to process *control image*. So Adapter will have a shorter training time, smaller model size but slighly lower image quality/control over generated images.
2. Adapter model will only be run once during enitre diffusion process no matter how many sample steps you set, which means Adapter will inference much faster compare to ControlNet and you can easily run multiple adapter at the same time on consumer grade GPU.
3. There exist a special variation of T2I-Adapter, Style-Adapter, that first make use of CLIP vision encoder to process *control image* first then adapter.


## StableDiffusionAdapterPipeline
[[autodoc]] StableDiffusionAdapterPipeline
	- all
	- __call__
	- enable_attention_slicing
	- disable_attention_slicing
	- enable_vae_slicing
	- disable_vae_slicing
	- enable_xformers_memory_efficient_attention
	- disable_xformers_memory_efficient_attention
